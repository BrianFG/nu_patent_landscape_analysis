{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "import nltk\n",
    "import string\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/outputs/autonomous_clean.pkl\"\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean text. Since text is on web (HTML) format, we can use BeautifulSoup to parse it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup(text):\n",
    "    text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "    return text\n",
    "df['text'] = df['text'].apply(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of words in patent abstract that do not inform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_list = [\n",
    "'claims','claim', 'method', 'provide', 'provided', \\\n",
    "'device', 'devices','apparatus','system', 'systems', \\\n",
    "'apparatuses', 'embodiments', 'embodiment','examples', \\\n",
    "'example','inventions', 'invention', 'present', \\\n",
    "'includes', 'include', 'including','description', \\\n",
    "'user', 'body', 'power', 'person', 'persons', \\\n",
    "'comprising', 'comprise', 'comprises', 'configured', \\\n",
    "'configure','for example', 'discloses', 'disclose', \\\n",
    "'method', 'said', 'abstract', 'abstracts', 'disclosed', 'herein', \\\n",
    "'autonomous', 'vehicle', 'self-driving', 'sensor'\n",
    "]\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "#larger list containing all custom stop words as well as from NLP libraries\n",
    "stop = set(list(stop_list) + list(ENGLISH_STOP_WORDS) \\\n",
    "           + stopwords.words('english') + punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    #clean text using regex\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\",\\\n",
    "                  \"\\t\", \"'m\", \"'ll\", '^\\d+\\s|\\s\\d+\\s|\\s\\d+$']\n",
    "    for i in separators:\n",
    "        text = re.sub(i, \" \", text.lower())\n",
    "    # Use Spacy to parse text    \n",
    "    doc = nlp(text)\n",
    "    # Lemmatization\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    # Remove stop words\n",
    "    tokens = [tok for tok in tokens if len(tok) != 1 and tok not in stop]\n",
    "    return tokens\n",
    "\n",
    "def text_processing(corp):\n",
    "    corp = tokenizeText(corp)\n",
    "    return ' '.join(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text processing to every row in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa9305abf4c433e8a154d4cd8568e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priority date\n",
       "2016-04-07    facilitate control steering accordance wheel p...\n",
       "2017-11-22    control obstacle management receive sense datu...\n",
       "2017-09-22    relate motion detection use radar technology i...\n",
       "2013-06-21    implementation detect closure lane shift lane ...\n",
       "2014-03-21    continuously record driving safety driver assi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = \"./data/outputs/autonomous_tokenized.pkl\"\n",
    "df.to_pickle(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
